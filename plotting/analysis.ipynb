{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c04169f2f665707",
   "metadata": {},
   "source": [
    "# The Enigma of Task Decomposition in Neural Networks\n",
    "Framing:\n",
    "  - Teaching Neural Networks Task Decomposition remains widely unsolved (Subgoal model predicts subgoals with small errors that the Synthesizer model omits, but synth model also knows how to decompose)\n",
    "  - Sampling is an alternative way to decompose tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:51:11.241190Z",
     "start_time": "2025-01-28T13:51:11.205852Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.core.pylabtools import figsize\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import tikzplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('retina') # high-res display in notebook\n",
    "\n",
    "# Set the Seaborn theme\n",
    "context = 'paper'  # paper, talk, or notebook\n",
    "\n",
    "save_settings = {'format': 'svg'}  # always use svg as it allows scaling figures up & down without losing resolution! \n",
    "\n",
    "if context == 'talk':\n",
    "    pal = sns.color_palette('deep')\n",
    "    PALETTE = {\"TIIPS\": \"#115e17\", \"ExeDec\": \"#a2af9f\", \"Ground truth\": \"#6e7a6c\", 'No-Subgoal Ablation': \"#0094c2\", 'Baseline': \"#00618C\", 'Both': \"#6e7a6c\", 'MLITS Only': \"#115e17\", \"ExeDec Only\": \"#a2af9f\", \"No-Subgoal Ablation Only\":\"#0094c2\", 'Both - MLITS': \"#115e17\", \"Both - ExeDec\": \"#a2af9f\", \"Both - No-Subgoal Ablation\": \"#0094c2\"}\n",
    "    sns.set_theme(context='talk', style='whitegrid', palette=sns.color_palette(PALETTE.values()), font_scale=2)\n",
    "else:\n",
    "    pal = sns.color_palette('colorblind')\n",
    "    PALETTE = {\"TIIPS\": \"#115e17\", \"ExeDec\": \"#a2af9f\", \"Ground truth\": \"#6e7a6c\", 'No-Subgoal Ablation': \"#0094c2\", 'Baseline': \"#00618C\", 'Both': \"#6e7a6c\", 'MLITS Only': \"#115e17\", \"ExeDec Only\": \"#a2af9f\", \"No-Subgoal Ablation Only\":\"#0094c2\", 'Both - MLITS': \"#115e17\", \"Both - ExeDec\": \"#a2af9f\", \"Both - No-Subgoal Ablation\": \"#0094c2\"}\n",
    "    sns.set_theme(context=context, style='whitegrid', palette=sns.color_palette(PALETTE.values()), font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16b419d8de267b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:51:11.258441Z",
     "start_time": "2025-01-28T13:51:11.252794Z"
    }
   },
   "outputs": [],
   "source": [
    "SEEDS = [10, 20, 30, 40, 50]\n",
    "SYNTH_MODEL = \"synthesizer_model\"\n",
    "EXPERIMENTS = [\"NONE\", \"LENGTH_GENERALIZATION\", \"COMPOSE_DIFFERENT_CONCEPTS\", \"SWITCH_CONCEPT_ORDER\", \"COMPOSE_NEW_OP\", \"ADD_OP_FUNCTIONALITY\"]\n",
    "EXP_LBL_MAP = {\"NONE\": 'Test on training distribution', \"LENGTH_GENERALIZATION\": 'Length generalization', \"COMPOSE_DIFFERENT_CONCEPTS\": 'Compose different concepts', \"SWITCH_CONCEPT_ORDER\": 'Switch concept order', \"COMPOSE_NEW_OP\": 'Compose new operation', \"ADD_OP_FUNCTIONALITY\": 'Add operation functionality'}\n",
    "PT_MAPPING = {\"separate\": \"ExeDec\", \"baseline\": \"Baseline\", 'tiips': 'TIIPS'}\n",
    "DATASET = {\"robustfill\": \"String manipulation\", \"deepcoder\": \"List manipulation\"}\n",
    "\n",
    "HANDLES = [\n",
    "        mpatches.Patch(color=\"#115e17\", label=\"TIIPS\"),\n",
    "        mpatches.Patch(color=\"#115e17\", label=\"Baseline\"),\n",
    "        mpatches.Patch(color=\"#a2af9f\", label=\"ExeDec\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4684b3afeb186ea",
   "metadata": {},
   "source": [
    "### Method Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e0db2011b31745",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:51:12.010773Z",
     "start_time": "2025-01-28T13:51:11.981656Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_length(solution):\n",
    "    subprograms = solution.split(' | ')\n",
    "    num_subprograms = len(subprograms)\n",
    "    num_inputs = sum(1 for sub in subprograms if 'INPUT' in sub)\n",
    "    length = num_subprograms - num_inputs\n",
    "    return length\n",
    "\n",
    "def load_decomposition_data(ds, key, task_index=True, threshold=1000):\n",
    "    path = f\"./tiips_results/evaluation/{ds}_e2e_predict_1/end_to_end_predict-{ds}-run-e2e_predict_1-\"\n",
    "    results = {\"TIIPS\": {s: {k: [] * 5 for k in EXPERIMENTS} for s in SEEDS}, \"ExeDec\": {s: {k: [] * 5 for k in EXPERIMENTS} for s in SEEDS}, \"Baseline\": {s: {k: [] * 5 for k in EXPERIMENTS} for s in SEEDS}}\n",
    "    \n",
    "    for prediction_type in [\"tiips\", \"baseline\", 'separate']:\n",
    "        pt = prediction_type\n",
    "        for experiment in EXPERIMENTS:\n",
    "            \n",
    "            for j, seed in enumerate(SEEDS):\n",
    "                pt = PT_MAPPING[prediction_type]\n",
    "\n",
    "                try:\n",
    "                    with open(path + prediction_type  + f'-/tb/hparams-dataset_type={ds},prediction_type={prediction_type},experiment={experiment},beam_size=10,seed={seed}/results-{prediction_type}.json') as f:\n",
    "                        data = json.load(f)\n",
    "                except FileNotFoundError:\n",
    "                    continue    \n",
    "\n",
    "                if len(data) != threshold:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    for task in data:\n",
    "                        r = task[key]\n",
    "                        if task['success'] and get_length(task['solution']) <= 1:\n",
    "                            r = np.nan\n",
    "                        if prediction_type == 'separate':\n",
    "                            if r:\n",
    "                                if task[key] > get_length(task['solution']) and task['success']:\n",
    "                                    r = get_length(task['solution'])\n",
    "                                elif not task['success'] and task[key] > get_length(task['ground_truth']):\n",
    "                                    r = get_length(task['ground_truth'])\n",
    "                                \n",
    "                            results[PT_MAPPING[prediction_type]][seed][experiment].append({\"success\": task['success'], \"subgoal_success\": r / get_length(task['solution']) if task['success'] else r / get_length(task['ground_truth']), 'subprogram_success_rate': sum([1 if a == b else 0 for a, b in zip(task['solution'].split(' | '), task['ground_truth'].split(' | '))]) / len(task['solution'].split(' | ')), 'Task': task['test_example_index'] if task_index else task, 'Subgoal Error': task['subgoal_first_error']})\n",
    "                        else:\n",
    "                            results[PT_MAPPING[prediction_type]][seed][experiment].append({\"success\": task['success'], \"subgoal_success\": task['synth_subgoal_success'] / get_length(task['solution']) if task['success'] else task['synth_subgoal_success'] / get_length(task['ground_truth']), 'subprogram_success_rate': sum([1 if a == b else 0 for a, b in zip(task['solution'].split(' | '), task['ground_truth'].split(' | '))]) / len(task['solution'].split(' | ')), 'Task': task['test_example_index'] if task_index else task, 'Subgoal Error': -1})\n",
    "                except KeyError:\n",
    "                    continue\n",
    "      \n",
    "    df_list = []\n",
    "    for pred_type, subdict in results.items():\n",
    "        for s, subsubdict in subdict.items():\n",
    "            for exp, values in subsubdict.items():\n",
    "                for value in values:\n",
    "                    df_list.append({'Approach': pred_type, 'Success': value[\"success\"], 'Correct Subgoals': value['subgoal_success'] * 100, 'Correct Subprograms': value['subprogram_success_rate'] * 100, 'Experiment': exp, 'Seed': s, 'Task': value['Task'], 'Subgoal First Error': value['Subgoal Error']})\n",
    "    \n",
    "    df = pd.DataFrame(df_list)\n",
    "    return df\n",
    "\n",
    "import re\n",
    "\n",
    "def load_performance_data(ds, threshold=1000):\n",
    "    path = f\"./tiips_results/evaluation/{ds}_e2e_predict_1/end_to_end_predict-{ds}-run-e2e_predict_1-\"\n",
    "    accuracies = {\"Baseline\": [], \"ExeDec\": {}, \"TIIPS\": {}}\n",
    "    num_steps_sp = {\"separate\": [], \"gt\": []}\n",
    "    num_steps_bu = {\"baseline\": [], \"gt\": []}\n",
    "    num_steps_ml = {\"tiips\": [], \"gt\": []}\n",
    "    for prediction_type in [\"tiips\", \"baseline\", 'separate']:\n",
    "        exp = {k: [np.nan] * 5 for k in EXPERIMENTS}\n",
    "        for experiment in EXPERIMENTS:\n",
    "            for j, seed in enumerate(SEEDS):\n",
    "\n",
    "                try:\n",
    "                    with open(path + prediction_type  + f'-/tb/hparams-dataset_type={ds},prediction_type={prediction_type},experiment={experiment},beam_size=10,seed={seed}/results-{prediction_type}.json') as f:\n",
    "                        data = json.load(f)\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "\n",
    "                exp[experiment][j] = len([ele for ele in data if ele[\"success\"]]) / len(data) * 100 if len(data) == threshold else np.nan\n",
    "                \n",
    "                if prediction_type == \"separate\": \n",
    "                    num_steps_sp[prediction_type] += [get_length(ele[\"solution\"]) for ele in data if ele[\"success\"]]\n",
    "                    num_steps_sp[\"gt\"] += [ele[\"ground_truth_length\"] for ele in data if ele[\"success\"]]\n",
    "                elif prediction_type == \"baseline\": \n",
    "                    num_steps_bu[prediction_type] += [get_length(ele[\"solution\"]) for ele in data if ele[\"success\"]]\n",
    "                    num_steps_bu[\"gt\"] += [ele[\"ground_truth_length\"] for ele in data if ele[\"success\"]]\n",
    "                elif prediction_type == \"tiips\": \n",
    "                    num_steps_ml[prediction_type] += [get_length(ele[\"solution\"]) for ele in data if ele[\"success\"]]\n",
    "                    num_steps_ml[\"gt\"] += [ele[\"ground_truth_length\"] for ele in data if ele[\"success\"]]\n",
    "            k = PT_MAPPING[prediction_type]\n",
    "        accuracies[k] = exp\n",
    "    return accuracies\n",
    "\n",
    "def extract_operations(program):\n",
    "    \"\"\"\n",
    "    Extracts operations used in the program by splitting at '|' and removing variables.\n",
    "    \n",
    "    Args:\n",
    "        program (str): The program as a string.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of cleaned operations found in the program.\n",
    "    \"\"\"\n",
    "    operations = []\n",
    "    for part in program.split('|'):\n",
    "        # Remove variable assignments and usages (e.g., 'x0 = ', 'x1', etc.)\n",
    "        cleaned_part = re.sub(r'\\bx\\d+\\b|=', '', part).strip()\n",
    "        if cleaned_part != 'INPUT':\n",
    "            operations.append(cleaned_part)\n",
    "    return operations\n",
    "\n",
    "def is_prefix_in_set(operation, operation_set):\n",
    "    \"\"\"\n",
    "    Checks if the operation starts with any prefix in the operation_set.\n",
    "    \n",
    "    Args:\n",
    "        operation (str): The operation to check.\n",
    "        operation_set (set): A set of operation prefixes.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the operation starts with any prefix in the operation_set, False otherwise.\n",
    "    \"\"\"\n",
    "    for op in operation_set:\n",
    "        if operation.startswith(op):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def round_to_two_significant_digits(x):\n",
    "    if x == 0 or x == np.nan:\n",
    "        return 0\n",
    "    else:\n",
    "        from math import log10, floor\n",
    "        return round(x, -int(floor(log10(abs(x)))) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a6a21f416379b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:51:12.861259Z",
     "start_time": "2025-01-28T13:51:12.852883Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace(original_string, old, new, count):\n",
    "    # Split the string into two parts: before and after the nth occurrence\n",
    "    parts = original_string.split(old)  # Limit the number of splits to `n`\n",
    "    if count < 3:\n",
    "        return original_string.replace(' ', '\\n')\n",
    "    \n",
    "    return parts[0] + ' ' + '\\n'.join(parts[1:])\n",
    "\n",
    "# Normalize marginal KDE plots\n",
    "def normalize_marginals_kde(g, data_d, x, y, hue, palette=PALETTE):\n",
    "    categories = data_d[hue].unique()\n",
    "    \n",
    "    for category in categories:\n",
    "        subset = data_d[data_d[hue] == category]\n",
    "        joint = sns.kdeplot(\n",
    "            data=subset,\n",
    "            x=\"Correct Subgoals\",\n",
    "            y=\"Correct Subprograms\",\n",
    "            ax=g.ax_joint,\n",
    "            fill=True,\n",
    "            alpha=0.6,\n",
    "            label=category,\n",
    "            clip=((0, 100), (0, 100)),\n",
    "            common_norm=True,\n",
    "            color=palette[category]\n",
    "        )\n",
    "    # Marginal Y\n",
    "    for category in categories:\n",
    "        subset = data_d[data_d[hue] == category]\n",
    "        sns.kdeplot(\n",
    "            y=subset[y],\n",
    "            ax=g.ax_marg_y,\n",
    "            fill=True,\n",
    "            label=f\"{category} (X)\",\n",
    "            common_norm=True,  # Normalize densities to sum to 1\n",
    "            alpha=0.4,\n",
    "            clip=(0, 100),\n",
    "            color=palette[category]\n",
    "        )\n",
    "        \n",
    "    # Marginal X\n",
    "    for category in categories:\n",
    "        subset = data_d[data_d[hue] == category]\n",
    "        sns.kdeplot(\n",
    "            x=subset[x],\n",
    "            ax=g.ax_marg_x,\n",
    "            fill=True,\n",
    "            label=f\"{category} (X)\",\n",
    "            common_norm=True,  # Normalize densities to sum to 1\n",
    "            alpha=0.4,\n",
    "            clip=(0, 100), \n",
    "            color=palette[category]\n",
    "        )\n",
    "    return joint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79547cdf7650c",
   "metadata": {},
   "source": [
    "# Qualitative analysis\n",
    "Plot intent match, i.e., the (post-hoc) overlap with subtask outputs, and syntactic overlap, i.e., the overlap with subprograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1e0ade61920a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T14:00:25.929737Z",
     "start_time": "2025-01-28T13:59:57.541763Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'deepcoder'\n",
    "df = load_decomposition_data(dataset, \"num_subgoal_success\", threshold=10)\n",
    "app = 'TIIPS'\n",
    "filtered_tasks = df[df['Approach'] == app]\n",
    "solved_tasks = filtered_tasks[filtered_tasks['Success'] == True]\n",
    "\n",
    "g = sns.jointplot(solved_tasks, x=\"Correct Subgoals\", y=\"Correct Subprograms\", fill=True,  kind='kde', clip=((0, 100), (0, 100)), color=PALETTE[app])\n",
    "g.ax_marg_x.set_title(f'Density of solved tasks: {app}')\n",
    "plt.ylabel('Syntactic overlap per task [%]')\n",
    "plt.xlabel('Intent match per task [%]')\n",
    "plt.ylim((0, 100))\n",
    "plt.xlim((0, 100))\n",
    "plt.tight_layout()\n",
    "\n",
    "app = 'ExeDec'\n",
    "filtered_tasks = df[df['Approach'] == app]\n",
    "solved_tasks = filtered_tasks[filtered_tasks['Success'] == True]\n",
    "\n",
    "g = sns.jointplot(solved_tasks, x=\"Correct Subgoals\", y=\"Correct Subprograms\", fill=True,  kind='kde', clip=((0, 100), (0, 100)), color=PALETTE[app])\n",
    "g.ax_marg_x.set_title(f'Density of solved tasks: {app}')\n",
    "plt.ylabel('Syntactic overlap per task [%]')\n",
    "plt.xlabel('Intent match per task [%]')\n",
    "plt.ylim((0, 100))\n",
    "plt.xlim((0, 100))\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2012fed2ac816e0",
   "metadata": {},
   "source": [
    "# Baseline Effect\n",
    "When repeatedly invoking ExeDec's Synthesizer Model, similar results can be achieved on the list manipulation domain as reported for ExeDec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd2222186691437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:52:26.658505Z",
     "start_time": "2025-01-28T13:52:25.844814Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'deepcoder'\n",
    "p = f\"./tiips_results/evaluation/{dataset}_e2e_predict_1/end_to_end_predict-{dataset}-run-e2e_predict_1-\"\n",
    "exp = {k: [np.nan] * 5 for k in EXPERIMENTS}\n",
    "num_steps = {e: {\"ExeDec\": {\"ExeDec\": [], \"Ground truth\": []}, \"Baseline\": {\"Baseline\": [], \"Ground truth\": []}, \"TIIPS\": {\"TIIPS\": [], \"Ground truth\": []}} for e in EXPERIMENTS}\n",
    "sols = {\"ExeDec\": {e: {\"Matches\": [], \"Mismatches\": []} for e in EXPERIMENTS}, \"Baseline\": {e: {\"Mismatches\": [], \"Matches\": []} for e in EXPERIMENTS}, \"TIIPS\": {e: {\"Mismatches\": [], \"Matches\": []} for e in EXPERIMENTS}}\n",
    "for experiment in EXPERIMENTS:\n",
    "    for j, seed in enumerate([10, 20, 30, 40, 50]):\n",
    "        for prediction_type in [\"separate\", \"tiips\", \"baseline\"]:\n",
    "            pt = PT_MAPPING[prediction_type]\n",
    "            try:\n",
    "                with open(p + prediction_type + f'-/tb/hparams-dataset_type={dataset},prediction_type={prediction_type},experiment={experiment},beam_size=10,seed={seed}/results-{prediction_type}.json') as f:\n",
    "                    data = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            \n",
    "            exp[experiment][j] = len([ele for ele in data if ele[\"success\"]]) / len(data) * 100 if len(data) == 1000 else np.nan\n",
    "            num_steps[experiment][pt][pt] += [get_length(ele[\"solution\"]) for ele in data if ele[\"success\"]]\n",
    "            num_steps[experiment][pt][\"Ground truth\"] += [ele[\"ground_truth_length\"] for ele in data if ele[\"success\"]]\n",
    "\n",
    "            if prediction_type == \"separate\" or prediction_type == \"baseline\": \n",
    "                match_ratios = []\n",
    "                for s, g in zip([ele[\"solution\"] for ele in data if ele[\"success\"]], [ele[\"ground_truth\"] for ele in data if ele[\"success\"]]):\n",
    "                    s_parts = s.split(' | ')\n",
    "                    g_parts = g.split(' | ')\n",
    "                    match_ratios.append(sum(1 for sp, gp in zip(s_parts, g_parts) if sp == gp) / len(s_parts))\n",
    "                sols[pt][experiment]['Matches'].append(match_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c84913bd4e041b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:52:30.201768Z",
     "start_time": "2025-01-28T13:52:26.790301Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "rows = []\n",
    "for pt, data in num_steps.items():\n",
    "    for category, metrics in data.items():\n",
    "        for key, values in metrics.items():\n",
    "            for value in values:\n",
    "                rows.append({\"Approach\": category, \"Category\": replace(EXP_LBL_MAP[pt], ' ', '\\n', EXP_LBL_MAP[pt].count(' ')), \"Legend\": key, \"Value\": value})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "overall_df = pd.DataFrame(rows)\n",
    "means = overall_df.groupby(['Approach', 'Category', 'Legend']).mean()\n",
    "stddevs = overall_df.groupby(['Approach', 'Category', 'Legend']).std() / np.sqrt(overall_df.groupby(['Approach', 'Category', 'Legend']).count())\n",
    "\n",
    "fig_width = 20\n",
    "z_value = norm.ppf(0.5 + 0.95 / 2)\n",
    "\n",
    "for m, app in enumerate(list(overall_df['Approach'].unique())):\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(fig_width, 6))# , gridspec_kw={'width_ratios': [fig_width / 6] * 6})\n",
    "    df = overall_df[overall_df['Approach'] == app]\n",
    "    for j, category in enumerate(list(df[\"Category\"].unique())):\n",
    "        axs[j] = sns.barplot(\n",
    "            data=df[df[\"Category\"] == category],\n",
    "            x=\"Category\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Legend\",\n",
    "            dodge=True,\n",
    "            ax=axs[j],\n",
    "            palette=PALETTE\n",
    "        )\n",
    "        if j == 0:\n",
    "            axs[j].set_ylabel(\"Number of decompositions\")\n",
    "        else:\n",
    "            axs[j].set_ylabel(\"\")\n",
    "        axs[j].set_xlabel(\"\")\n",
    "        axs[j].set_ylim((0, means.max()['Value'] + 1.5))\n",
    "        if j != 5:\n",
    "            axs[j].get_legend().remove()\n",
    "        else:\n",
    "            legend = axs[j].legend(loc=\"upper right\", ncol=1, fontsize=16)\n",
    "        # axs[j].tick_params('x', labelsize=10)\n",
    "        for n, lgd in enumerate([app, 'Ground truth']):\n",
    "            ci_upper = means.loc[(app, category, lgd), 'Value'] + z_value *  + stddevs.loc[(app, category, lgd), 'Value']\n",
    "            axs[j].text(-4/fig_width if n == 0 else 4/fig_width, ci_upper + 0.1, f\"{means.loc[(app, category, lgd), 'Value']:.1f}\", \n",
    "                 ha='center', va='bottom', fontweight='bold', color='black', fontsize=16)\n",
    "    for ax in axs:\n",
    "        legend = ax.get_legend()\n",
    "        if legend is not None and not hasattr(legend, \"_ncol\"):\n",
    "            legend._ncol = legend._ncols if hasattr(legend, \"_ncols\") else 1\n",
    "    fig.suptitle(f\"Number of decompositions by {app}\")\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f74de",
   "metadata": {},
   "source": [
    "# Quantitative Performance\n",
    "TIIPS outperforms ExeDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f564a5b9ea4b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:52:52.161837Z",
     "start_time": "2025-01-28T13:52:39.061035Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'deepcoder'\n",
    "accuracies = load_performance_data(dataset, threshold=10)\n",
    "\n",
    "# Transform data into a DataFrame suitable for seaborn plotting\n",
    "df_list = []\n",
    "for hue_key, subdict in accuracies.items():\n",
    "    for category, values in subdict.items():\n",
    "        for value in values:\n",
    "            df_list.append({'Category': replace(EXP_LBL_MAP[category], ' ', '\\n', EXP_LBL_MAP[category].count(' ')), 'Value': value, 'Type': hue_key})\n",
    "\n",
    "df = pd.DataFrame(df_list)\n",
    "df = df[df['Type'] != 'No-Subgoal Ablation']\n",
    "\n",
    "# Filter out the 'None' category for overall average calculation\n",
    "df_no_none = df[df['Category'] != 'Test on\\ntraining\\ndistribution']\n",
    "# df_no_none = df_no_none[df_no_none['Category'] != 'Length\\ngeneralization']\n",
    "# df_no_none = df_no_none[df_no_none['Category'] != 'Compose\\nnew\\noperation']\n",
    "\n",
    "# Calculate overall average across all categories except 'None' for each Type\n",
    "overall_averages = df_no_none.groupby('Type')['Value'].mean().reset_index()\n",
    "overall_averages['Category'] = 'Generalization\\naverage'\n",
    "\n",
    "# Append the overall average to the DataFrame\n",
    "df = pd.concat([df, overall_averages], ignore_index=True)\n",
    "\n",
    "# Calculate average accuracy for each Category-Type combination\n",
    "averages = df.groupby(['Category', 'Type'])['Value'].mean().reset_index()\n",
    "averages['std'] = df.groupby(['Category', 'Type'])['Value'].std().reset_index()['Value']\n",
    "averages['std'] = averages['std'].fillna(0)\n",
    "\n",
    "# Create the barplot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax = sns.barplot(data=df, x='Category', y='Value', hue='Type', palette=PALETTE, ax=ax)\n",
    "for line in ax.lines:\n",
    "    line.set_color(\"gray\")\n",
    "\n",
    "hue_types = df['Type'].unique()\n",
    "num_hues = len(hue_types)\n",
    "offsets = {hue_type: -0.2725 + 0.545 / (num_hues - 1) * i for i, hue_type in enumerate(hue_types)}\n",
    "\n",
    "# Add average accuracy annotations with dynamic offsets\n",
    "for i, row in averages.iterrows():\n",
    "    category = row['Category']\n",
    "    avg_value = row['Value']\n",
    "    std = row['std']\n",
    "    type_ = row['Type']\n",
    "    x = list(df['Category'].unique()).index(category)  # x position for the category\n",
    "    \n",
    "    text = f\"{avg_value:.1f}\" # if avg_value < 10 else f\" {avg_value:.1f}\"\n",
    "    \n",
    "    plt.text(x + offsets[type_], avg_value + 1.1 * std, f\"{avg_value:.2g}\", \n",
    "             ha='center', va='bottom', fontweight='bold', color='black', fontsize=12)\n",
    "\n",
    "\n",
    "plt.title(f\"Out-of-distribution generalization results on the {DATASET[dataset].lower()} domain\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"End-to-end test accuracy [%]\")\n",
    "fig.gca().set_ylim(0, 102)\n",
    "plt.tight_layout()\n",
    "ax.legend().set_title('')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(0.91, 1.0))\n",
    "legend = ax.get_legend()\n",
    "if legend is not None and not hasattr(legend, \"_ncol\"):\n",
    "    legend._ncol = legend._ncols if hasattr(legend, \"_ncols\") else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "537170f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'deepcoder'\n",
    "\n",
    "exp = {k: [np.nan] * 5 for k in EXPERIMENTS}\n",
    "num_steps = {e: {\"ExeDec\": {\"ExeDec\": [], \"Ground truth\": []}, \"Baseline\": {\"Baseline\": [], \"Ground truth\": []}, \"TIIPS\": {\"TIIPS\": [], \"Ground truth\": []}} for e in EXPERIMENTS}\n",
    "sols = {\"ExeDec\": {e: {\"Matches\": [], \"Mismatches\": []} for e in EXPERIMENTS}, \"Baseline\": {e: {\"Mismatches\": [], \"Matches\": []} for e in EXPERIMENTS}, \"TIIPS\": {e: {\"Mismatches\": [], \"Matches\": []} for e in EXPERIMENTS}}\n",
    "for experiment in EXPERIMENTS:\n",
    "    for j, seed in enumerate([10, 20, 30, 40, 50]):\n",
    "        for prediction_type in [\"separate\", \"tiips\", \"baseline\"]:\n",
    "            pt = PT_MAPPING[prediction_type]\n",
    "            p = f\"tiips_results/evaluation/{dataset}_e2e_predict_1/end_to_end_predict-{dataset}-run-e2e_predict_1-\"\n",
    "            try:\n",
    "                with open(p + prediction_type + f'-/tb/hparams-dataset_type={dataset},prediction_type={prediction_type},experiment={experiment},beam_size=10,seed={seed}/results-{prediction_type}.json') as f:\n",
    "                    data = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "            \n",
    "            exp[experiment][j] = len([ele for ele in data if ele[\"success\"]]) / len(data) * 100 if len(data) == 1000 else np.nan\n",
    "            num_steps[experiment][pt][pt] += [ele['num_steps'] if prediction_type == 'separate-' else get_length(ele['solution']) for ele in data if ele[\"success\"]]\n",
    "            num_steps[experiment][pt][\"Ground truth\"] += [ele[\"ground_truth_length\"] for ele in data if ele[\"success\"]]\n",
    "\n",
    "            if prediction_type == \"separate\" or prediction_type == \"baseline\":\n",
    "                match_ratios = []\n",
    "                for s, g in zip([ele[\"solution\"] for ele in data if ele[\"success\"]], [ele[\"ground_truth\"] for ele in data if ele[\"success\"]]):\n",
    "                    s_parts = s.split(' | ')\n",
    "                    g_parts = g.split(' | ')\n",
    "                    match_ratios.append(sum(1 for sp, gp in zip(s_parts, g_parts) if sp == gp) / len(s_parts))\n",
    "                sols[pt][experiment]['Matches'].append(match_ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283456a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "rows = []\n",
    "for pt, data in num_steps.items():\n",
    "    for category, metrics in data.items():\n",
    "        for key, values in metrics.items():\n",
    "            for value in values:\n",
    "                rows.append({\"Approach\": category, \"Category\": replace(EXP_LBL_MAP[pt], ' ', '\\n', EXP_LBL_MAP[pt].count(' ')), \"Legend\": key, \"Value\": value})\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "overall_df = pd.DataFrame(rows)\n",
    "overall_df = overall_df[overall_df['Legend'] != 'Ground truth']\n",
    "means = overall_df.groupby(['Approach', 'Category', 'Legend']).mean()\n",
    "stddevs = overall_df.groupby(['Approach', 'Category', 'Legend']).std() / np.sqrt(overall_df.groupby(['Approach', 'Category', 'Legend']).count())\n",
    "\n",
    "fig_width = 20\n",
    "z_value = norm.ppf(0.5 + 0.95 / 2)\n",
    "\n",
    "#for m, app in enumerate(list(overall_df['Approach'].unique())):\n",
    "fig, axs = plt.subplots(1, 6, figsize=(fig_width, 6))# , gridspec_kw={'width_ratios': [fig_width / 6] * 6})\n",
    "df = overall_df# [overall_df['Approach'] == app]\n",
    "for j, category in enumerate(list(df[\"Category\"].unique())):\n",
    "    axs[j] = sns.barplot(\n",
    "        data=df[df[\"Category\"] == category],\n",
    "        x=\"Category\",\n",
    "        y=\"Value\",\n",
    "        hue=\"Legend\",\n",
    "        dodge=True,\n",
    "        ax=axs[j],\n",
    "        palette=PALETTE\n",
    "    )\n",
    "    if j == 0:\n",
    "        axs[j].set_ylabel(\"Number of guidance calls\")\n",
    "    else:\n",
    "        axs[j].set_ylabel(\"\")\n",
    "    axs[j].set_xlabel(\"\")\n",
    "    axs[j].set_ylim((0, means.max()['Value'] + 1.5))\n",
    "    if j != 5:\n",
    "        axs[j].get_legend().remove()\n",
    "    else:\n",
    "        legend = axs[j].legend(loc=\"upper right\", ncol=1, fontsize=16)\n",
    "    # axs[j].tick_params('x', labelsize=10)\n",
    "    for n, app in enumerate(['ExeDec', 'TIIPS']):\n",
    "        ci_upper = means.loc[(app, category, app), 'Value'] + z_value *  + stddevs.loc[(app, category, app), 'Value']\n",
    "        axs[j].text(-4/fig_width if n == 0 else 4/fig_width, ci_upper + 0.1, f\"{means.loc[(app, category, app), 'Value']:.1f}\", \n",
    "                ha='center', va='bottom', fontweight='bold', color='black', fontsize=16)\n",
    "for ax in axs:\n",
    "    legend = ax.get_legend()\n",
    "    if legend is not None and not hasattr(legend, \"_ncol\"):\n",
    "        legend._ncol = legend._ncols if hasattr(legend, \"_ncols\") else 1\n",
    "fig.suptitle(f\"Number of calls to the transductive guidance model\")\n",
    "fig.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
